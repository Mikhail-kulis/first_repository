{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Построена модель, которая по данным с параметрами добычи и очистки предсказывает коэффициент восстановления золота из золотосодержащей руды. \n",
    "\n",
    "Задачи:\n",
    "\n",
    "- Подготовить данные;\n",
    "- Провести исследовательский анализ данных;\n",
    "- Построить и обучить модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                 \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math                                          # импорт библиотеки math\n",
    "from sklearn.metrics import mean_absolute_error      # импорт метода вычисления МАЕ\n",
    "from scipy import stats as st\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate # импорт методов разбиения на выборки\n",
    "# и кросс-валидации\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.dummy import DummyRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('...')   # чтение обучающей выборки\n",
    "test = pd.read_csv('...')     # чтение тестовой выборки\n",
    "data = pd.read_csv('...')     # чтение исходных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()                                          # вывод информации об обучающей выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "Тестовая выборка содержит незначительное число пропусков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rougher.input.feed_size'].to_csv('Sorting')                                   # вывод информации об обучающей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv('Sorting')\n",
    "display(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "Обучающая выборка содержит параметры с большим числом пропусков, например secondary_cleaner.output.tail_sol и др."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()                             # вывод информации об исходных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "Исходные данные тоже содержат параметры, в которых много пропусков.\n",
    "\n",
    "Кроме того, дата во всех датафреймах имеет тип object, что не есть хорошо, необходимо исправить тип данных на datetime. Также обнаружено, что датафрейм с тестовой выборкой содержит усечённое число признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Приведение данных к нужному типу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['date'] = pd.to_datetime(                           #\n",
    "    train['date'], format='%Y-%m-%d %H:%M:%S')            #\n",
    "data['date'] = pd.to_datetime(                            #    приведение признака 'date' к формату datetime\n",
    "    data['date'], format='%Y-%m-%d %H:%M:%S')             #               во всех трёх датафреймах\n",
    "test['date'] = pd.to_datetime(                            #\n",
    "    test['date'], format='%Y-%m-%d %H:%M:%S')             #\n",
    "train.info(verbose=False)                                 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка правильности указанной эффективности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recovery = train['rougher.output.concentrate_au'] * (train['rougher.input.feed_au'] -                              # расчёт эфф-\n",
    "                                                    train['rougher.output.tail_au']) / (                           # ективности\n",
    "train['rougher.input.feed_au'] * (train['rougher.output.concentrate_au'] - train['rougher.output.tail_au'])) * 100 # флотации\n",
    "\n",
    "MAE = mean_absolute_error(train['rougher.output.recovery'], recovery)        # МАЕ между посчитанной и указанной эффективности\n",
    "print(f\"МАЕ указанной и посчитанной эффективности после флотации: {MAE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "Указанная эффективность рассчитана верно, ведь МАЕ практически 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Признаки, отсутствующие в тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_columns = pd.Series(test.columns.symmetric_difference(train.columns))  # список отсутствующих колонок в тестовой выборке\n",
    "train[no_columns].info()                                                  # признаки в этих колонках в обучающей выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "Видно, что в тестовой выборке отсутствует много признаков, в том числе и целевой rougher.output.recovery. Все эти данные относятся к типу float. Необходимо удалить из обучающей выборки эти признаки, чтобы тест проходил правильно. Целевые признаки будут взяты из исходных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим целевые признаки в тестовую выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date = test['date']              # создадим переменную с датами из тестовой выборки\n",
    "test['rougher.output.recovery'] = data.query('date in @test_date')['rougher.output.recovery'].reset_index(drop = True) # срез из\n",
    "# исходных данных по датам из тестовой выборки и признаку эффективности флотации добавляем в тестовую выборку\n",
    "test['final.output.recovery'] = data.query('date in @test_date')['final.output.recovery'].reset_index(drop = True) # срез из\n",
    "# исходных данных по датам из тестовой выборки и признаку финальной эффективности добавляем в тестовую выборку\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим из обучающей вборки признаки, отсутствующие в тестовой для правильного теста будущей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_columns = pd.Series(test.columns.symmetric_difference(train.columns)) # список отсутствующих колонок в тестовой выборке\n",
    "train = train.drop(no_columns, axis = 1)            # удаление этих колонок в обучающей выборке\n",
    "print(f'Размеры обучающей выборки: {train.shape}') \n",
    "print(f'Размеры тестовой выборки: {test.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "Теперь обучающая и тестовая выборки имеют одинаковое число признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заполнение пропусков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполнять пропуски будем исходя из того факта, что измерения, проведённые в близкие времена должны быть похожи, поэтому в датафрейме, отсортированном по дате пропуск можно восстановить по соседним значениям.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train = pd.concat([test,train]).sort_values(by='date').reset_index() # объединение тестовой и обучающей выборки для запол\n",
    "# нения пропусков, поскольку нам необходимы данные с последовательным измененением времени при движении по датафрейму\n",
    "test_train.info(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train = test_train.fillna(method='ffill') # заполняем пропуски предыдущим значением признака\n",
    "test_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date = test['date']         # создадим переменную с датами из тестовой выборки\n",
    "train_date = train['date']       # создадим переменную с датами из обучающей выборки\n",
    "\n",
    "test =  test_train.query('date in @test_date').reset_index(drop=True).drop('index',axis=1)  # делаем срез по тем датам, \n",
    "# которые есть в тестовой выборке\n",
    "train = test_train.query('date in @train_date').reset_index(drop=True).drop('index',axis=1) # делаем срез по тем датам, \n",
    "# которые есть в обучающей выборке\n",
    "display(test.head())\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concentration(metal):                                      # функция подсчёта средней концентрации\n",
    "    concentr = []\n",
    "    for i in  data.columns:                                    # цикл по всем признакам\n",
    "        if 'concentrate_'+metal in i or 'feed_'+metal  in i:   # если признак содержит в названии концентрацию\n",
    "                                                               # и связан с выбранным металлом, то \n",
    "            concentr.append(data[i].mean())        # считаем среднюю концентрацию \n",
    "            \n",
    "    \n",
    "    a = pd.Series(concentr, index=[3,2,0,1]).sort_index() # располагаем концентрации в порядке технологического процесса\n",
    "    a.index = ['Сырьё','Черновой конц.','Конц. после 1 очистки', 'Конц. после 2 очистки']\n",
    "\n",
    "    return a\n",
    "\n",
    "plt.title(\"Золото\")\n",
    "concentration('au').plot(kind='bar').set_ylabel(\"Концентрация\")\n",
    "plt.show()\n",
    "plt.title(\"Серебро\")\n",
    "concentration('ag').plot(kind='bar').set_ylabel(\"Концентрация\")\n",
    "plt.show()\n",
    "plt.title(\"Свинец\")\n",
    "concentration('pb').plot(kind='bar').set_ylabel(\"Концентрация\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "По мере прохождения технологических циклов концентрация золота увеличивается, серебра - меняется не монотонно, свинца - растёт."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка распределений гранул сырья"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['rougher.input.feed_size'].hist(bins=100, range = (0,200),  density=True) # распределение гранул сырья в обучающей выборке\n",
    "test['rougher.input.feed_size'].hist(bins=100, range = (0,200), density=True) # распределение гранул сырья в тестовой выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выдвигаем гипотезу H0, что распределения похожи. H1 - распределения отличаются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05  # пороговое значение устанавливаем на 5% \n",
    "\n",
    "feed_size_train = train['rougher.input.feed_size']       \n",
    "feed_size_test = test['rougher.input.feed_size']\n",
    "\n",
    "results = st.ks_2samp(feed_size_train,feed_size_test)     # критерий Колмогорова-Смирнова\n",
    "\n",
    "\n",
    "print('p-значение: ', results.pvalue)                    # выводим p-value на экран\n",
    "\n",
    "if results.pvalue < alpha:                               # сравниваем p-value с пороговым значением\n",
    "    print(\"Отвергаем нулевую гипотезу\")\n",
    "else:\n",
    "    print(\"Не получилось отвергнуть нулевую гипотезу\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "Подозрения о том, что распределения отличаются подтвердились и критерием Колмогорова-Смирнова. Поэтому правильней, чтобы обучение и проверка модели проходили на других выборках. Но поскольку наша задача, всё-таки, использовать уже готовые выборки, придётся смириться с грядущей ошибкой в оценке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исследование поведения суммарной концентрации всех веществ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку в тестовой выборке представлены не все параметры, которые нужны для расчёта суммарной концентрации, то считать её мы будем из исходных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_concentration_1 = data['rougher.input.feed_ag'] + data['rougher.input.feed_au'] + \\\n",
    "data['rougher.input.feed_pb'] + data['rougher.input.feed_sol']               # расчёт суммарной концентрации в сырье\n",
    "sum_concentration_1.hist(bins=100, range = (0,100))                                              # построение гистограммы\n",
    "print(f'Средняя концентрация всех веществ сырья {sum_concentration_1.mean()}')                   \n",
    "\n",
    "\n",
    "sum_concentration_2 = data['rougher.output.concentrate_ag'] + data['rougher.output.concentrate_ag'] + \\\n",
    "data['rougher.output.concentrate_ag'] + data['rougher.output.concentrate_ag']         # расчёт суммарной концентрации в \n",
    "# черновом концентрате\n",
    "sum_concentration_2.hist(bins=100, range = (0,100))\n",
    "print(f'Средняя концентрация всех веществ чернового концентрата {sum_concentration_2.mean()}')\n",
    "\n",
    "sum_concentration = data['final.output.concentrate_ag'] + data['final.output.concentrate_au'] + \\\n",
    "data['final.output.concentrate_pb'] + data['final.output.concentrate_sol']   # расчёт суммарной концентрации в финальном \n",
    "# концентрате\n",
    "sum_concentration.hist(bins=100, range = (0,100))\n",
    "plt.legend(['Сырьё', 'Черновой концентрат','Готовый концентрат'])                               # создание легенды\n",
    "print(f'Средняя концентрация всех веществ готового концентрата {sum_concentration.mean()}')\n",
    "\n",
    "data['sum_concentration_feed'] = sum_concentration_1                #\n",
    "data['sum_concentration_rougher'] = sum_concentration_2             #   добавление этих новых метрик в исходные данные\n",
    "data['sum_concentration_final'] = sum_concentration                 #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "Из графика видно, что распределения концентраций всех веществ по трём стадиям сильно различаются. Самый узкий пик (самый малый разброс) имеет распределение концентрации готового сырья. Самый большой разброс имеет распределение концентраций чернового концентрата. Средняя концентрация веществ после флотации падает, а после двух этапов очистки - растёт.\n",
    "\n",
    "В распределениях обнаружены артефакты в виде длинных хвостов, соответствующих малым значениям концентрации или вообще нулевым значениям. Такие данные нужно удалить из рассмотрения. Границей адекватного значения концетрации было выбрано значение 20%, поскольку большая часть всех распределений лежит правее этой цифры. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление артефактов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вначале, удалим лишние данные из исходных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.query('sum_concentration_feed > 20 and sum_concentration_rougher > 20 and sum_concentration_final > 20') # убираем\n",
    "# данные в исходных данных, где параметры суммарной концентрации явно занижены\n",
    "data['sum_concentration_feed'].hist(bins=100, range = (0,100))       #\n",
    "data['sum_concentration_rougher'].hist(bins=100, range = (0,100))    # проверяем обрезку\n",
    "data['sum_concentration_final'].hist(bins=100, range = (0,100))      #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_date = data['date']      # создадим переменную, в которую положим дату объектов после обрезки\n",
    "test_before = test\n",
    "train_before = train\n",
    "test = test.query('date in @good_date').reset_index(drop=True)   # выберем в тестовой выборке только те\n",
    "# объекты, которые присутствуют в исходных данных после удаления артефактов\n",
    "train = train.query('date in @good_date').reset_index(drop=True) # выберем в обучающей выборке только те\n",
    "# объекты, которые присутствуют в исходных данных после удаления артефактов\n",
    "print(f'Размеры тестовой выборки до обрезки:{test_before.shape} и после: {test.shape}')\n",
    "print(f'Размеры обучающей выборки до обрезки: {train.shape} и после: {train.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "Все объекты с сильно заниженными значениями суммарной концентрации попали в тестовую выборку. Мы их из рассмотрения убрали."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция для расчёта sMAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из определения метрики видно, что в случае равенства 0 предсказанной величины и целевого признака возникает неопределённость 0/0. Это необходимо учитывать в расчёте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sMAPE(predictions,target):\n",
    "    for i in predictions.index:                          # цикл по всем предсказаниям\n",
    "        if predictions[i] == 0 and target[i] == 0:       # если возникает неопределённость 0/0, \n",
    "            predictions = predictions.drop(i)            # то убираем объект из рассмотрения\n",
    "            target = target.drop(i)                      #\n",
    "    quality = 1/len(predictions) * sum(abs(predictions-target)/((abs(predictions)+abs(target))/2))*100  # вычисдяем величину\n",
    "# sMAPE по указанной формуле\n",
    "    return quality\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train.drop(['date','final.output.recovery','rougher.output.recovery'], axis=1) # выделяем признаки\n",
    "target_rougher = train['rougher.output.recovery']                # выделяем первый целевой признак\n",
    "\n",
    "model_1 = LinearRegression()\n",
    "\n",
    "predictions = pd.Series(cross_val_predict(model_1, features, target_rougher, cv=10), index=target_rougher.index) # получаем \n",
    "# предсказания после кросс-валидации по 10 блокам\n",
    "\n",
    "\n",
    "print(f\"sMAPE прогноза эффективности обогащения чернового концентрата в модели лин. регрессии: \\\n",
    "{sMAPE(predictions, target_rougher)} %\")\n",
    "\n",
    "metrics_best = 100                         # делаем начальное значение метрики наихудшим       \n",
    "for depth in range(1,8):                   # цикл по глубине модели\n",
    "    model_2 = DecisionTreeRegressor(random_state=12345, max_depth=depth)    # модель решающего дерева \n",
    "    predictions = pd.Series(cross_val_predict(model_2, features, target_rougher, cv=10), index=target_rougher.index) # получаем \n",
    "# предсказания после кросс-валидации по 10 блокам\n",
    "    metrics = sMAPE(predictions, target_rougher)        # применение функции sMAPE и получение соотв. метрики\n",
    "    if metrics < metrics_best:      #\n",
    "        metrics_best = metrics      # поиск в теле цикла лучшей метрики и лучшего гиперпараметра глубины\n",
    "        best_depth = depth          #\n",
    "print(f\"sMAPE прогноза эффективности обогащения чернового концентрата в модели решающего дерева: {metrics_best} %\")\n",
    "print(f'Лучшая глубина решающего дерева {best_depth}')\n",
    "\n",
    "metrics_best = 100                         # делаем начальное значение метрики наихудшим\n",
    "for depth in range(1,8):\n",
    "    model_3 = RandomForestRegressor(random_state=12345, max_depth=depth)\n",
    "    predictions = pd.Series(cross_val_predict(model_3, features, target_rougher, cv=2), index=target_rougher.index) # получаем \n",
    "# предсказания после кросс-валидации по 2 блокам\n",
    "    metrics = sMAPE(predictions, target_rougher)       # применение функции sMAPE и получение соотв. метрики\n",
    "    if metrics < metrics_best:      #\n",
    "        metrics_best = metrics      # поиск в теле цикла лучшей метрики и лучшего гиперпараметра глубины\n",
    "        best_depth = depth          #\n",
    "\n",
    "print(f\"sMAPE прогноза эффективности обогащения чернового концентрата в модели случайного леса: {metrics_best} %\")\n",
    "print(f'Лучшая глубина случайного леса: {best_depth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_final = train['final.output.recovery']                # выделяем второй целевой признак\n",
    "\n",
    "predictions = pd.Series(cross_val_predict(model_1, features, target_final, cv=10), index=target_final.index) # получаем \n",
    "# предсказания после кросс-валидации по 10 блокам\n",
    "print(f\"sMAPE прогноза эффективности обогащения финального концентрата в модели лин. регрессии: {sMAPE(predictions, target_final)} %\")\n",
    "\n",
    "metrics_best = 100                 # делаем начальное значение метрики наихудшим \n",
    "for depth in range(1,8):\n",
    "    model_2 = DecisionTreeRegressor(random_state=12345, max_depth=depth)\n",
    "    predictions = pd.Series(cross_val_predict(model_2, features, target_final, cv=10), index=target_final.index) # получаем \n",
    "# предсказания после кросс-валидации по 10 блокам\n",
    "    metrics = sMAPE(predictions, target_final)     # применение функции sMAPE и получение соотв. метрики\n",
    "    if metrics < metrics_best:      #\n",
    "        metrics_best = metrics      # поиск в теле цикла лучшей метрики и лучшего гиперпараметра глубины\n",
    "        best_depth = depth          #\n",
    "print(f\"sMAPE прогноза эффективности обогащения финального концентрата в модели решающего дерева: {metrics_best} %\")\n",
    "print(f'Лучшая глубина решающего дерева {best_depth}')\n",
    "\n",
    "metrics_best = 100                  # делаем начальное значение метрики наихудшим\n",
    "for depth in range(1,6):\n",
    "    model_3 = RandomForestRegressor(random_state=12345, max_depth=depth)\n",
    "    predictions = pd.Series(cross_val_predict(model_3, features, target_final, cv=2), index=target_final.index) # получаем \n",
    "# предсказания после кросс-валидации по 2 блокам\n",
    "    metrics = sMAPE(predictions, target_rougher)  # применение функции sMAPE и получение соотв. метрики\n",
    "    if metrics < metrics_best:      #\n",
    "        metrics_best = metrics      # поиск в теле цикла лучшей метрики и лучшего гиперпараметра глубины\n",
    "        best_depth = depth          #\n",
    "print(f\"sMAPE прогноза эффективности обогащения финального концентрата в модели случайного леса: {sMAPE(predictions, target_final)} %\")\n",
    "print(f'Лучшая глубина случайного леса: {best_depth}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "В случае с прогнозом эффективности обогащения чернового концентрата лучшую метрику показывет модель линейной регрессии. Если же говорить о прогнозе эффективности обогащения финального концентрата, то здесь победителем стало решающее дерево с глубиной 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение лучших моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_1 = LinearRegression()\n",
    "best_model_1.fit(features, target_rougher)  # обучение на всей обучающей выборке с первым целевым признаком\n",
    "\n",
    "best_model_2 = DecisionTreeRegressor(random_state=12345, max_depth=5)  # модель решающего дерева с глубиной 5\n",
    "best_model_2.fit(features, target_final)    # обучение на всей обучающей выборке со вторым целевым признаком"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование модели и подсчёт итоговой метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_null =  test.dropna()\n",
    "\n",
    "features_test = test_null.drop(['date','final.output.recovery','rougher.output.recovery'], axis=1) # выделяем признаки\n",
    "target_test = test_null['rougher.output.recovery']                # выделяем первый целевой признак\n",
    "\n",
    "\n",
    "predictions = pd.Series(best_model_1.predict(features_test), index = target_test.index)\n",
    "sMAPE_rougher = sMAPE(predictions, target_test)                   # применение функции sMAPE и получение соотв. метрики\n",
    "\n",
    "\n",
    "\n",
    "target_test = test_null['final.output.recovery']                  # выделяем второй целевой признак\n",
    "\n",
    "\n",
    "predictions = pd.Series(best_model_2.predict(features_test), index = target_test.index)\n",
    "sMAPE_final = sMAPE(predictions, target_test)                     # применение функции sMAPE и получение соотв. метрики\n",
    "result = 0.25 * sMAPE_rougher + 0.75 * sMAPE_final                # расчёт sMAPE по указанной формуле\n",
    "print(f'Итоговая sMAPE: {result} %')\n",
    "\n",
    "\n",
    "target_test = test_null['rougher.output.recovery']                # выделяем первый целевой признак\n",
    "dummy_regr_1 = DummyRegressor()\n",
    "dummy_regr_1.fit(features, target_rougher)\n",
    "predictions_dummy_1 = pd.Series(dummy_regr_1.predict(features_test), index = target_test.index)\n",
    "sMAPE_rougher = sMAPE(predictions_dummy_1, target_test)\n",
    "\n",
    "target_test = test_null['final.output.recovery']                  # выделяем второй целевой признак\n",
    "dummy_regr_2 = DummyRegressor()\n",
    "dummy_regr_2.fit(features, target_final)\n",
    "predictions_dummy_2 = pd.Series(dummy_regr_2.predict(features_test), index = target_test.index)\n",
    "sMAPE_final = sMAPE(predictions_dummy_2, target_test)\n",
    "result_dummy = 0.25 * sMAPE_rougher + 0.75 * sMAPE_final                # расчёт sMAPE по указанной формуле\n",
    "print(f'Для dummy-модели итоговая sMAPE: {result_dummy} %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоговый вывод\n",
    "\n",
    "В ходе подготовки данных к построению модели была преобразована дата к нужному типу данных, из обучающей выборки удалены признаки, не встречающиеся в тестовой. Благодаря условию, что признаки двух объектов, полученных за близкое время, имеют близкие значения, был реализован метод восстановления пропусков: при встрече с пропуском берётся значение признака прошлого объекта.\n",
    "\n",
    "Анализ концетраций трёх металлов показал, что по мере прохождения технологического цикла, концентрация золота и свинца растёт, а концентрация серебра ведёт себя не монотонно.\n",
    "\n",
    "Проверка распределения размеров гранул сырья в тестовой и обучающей выборке с помощью применения критерия Колмогорова-Смирнова завершилась выводом: разбиение исходных данных было выполнено не оптимально, распределения отличаются (p-значение:  5.2e-213). Но сама форма распределений похожа. Конечно же, верней было бы провести переразбивку на две выборки, но у нас была задача отработать предсказания на конкретной выборке. \n",
    "\n",
    "Расчёт суммарной концентрации всех веществ на трёх этапах очистки и построение соответсвтующих распределений показали наличия артефактов: нулевых или очень малых значений суммарной концентрации (< 20%). От объектов с такими значениями нам пришлось избавиться. \n",
    "\n",
    "Для расчёта нашей метрики sMAPE была написана соответствующая функция, учитывающая возможность возникновения неопределённости 0/0.\n",
    "\n",
    "\n",
    "После проведения кросс-валидации для трёх различных моделей, оказалось, что для предсказаний двух метрик оптимальными оказались две разные модели. Для эффективности обогащения чернового концентрата - линейная регрессия (sMAPE = 6.0556 %), а для финального концентрата - решающее дерево с глубиной 5 (sMAPE = 8.695 %)\n",
    "\n",
    "Итоговое значение метрики sMAPE составило: 7.442 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист готовности проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [x]  Весь код выполняется без ошибок\n",
    "- [x]  Ячейки с кодом расположены в порядке выполнения\n",
    "- [x]  Выполнен шаг 1: данные подготовлены\n",
    "    - [x]  Проверена формула вычисления эффективности обогащения\n",
    "    - [x]  Проанализированы признаки, недоступные в тестовой выборке\n",
    "    - [x]  Проведена предобработка данных\n",
    "- [x]  Выполнен шаг 2: данные проанализированы\n",
    "    - [x]  Исследовано изменение концентрации элементов на каждом этапе\n",
    "    - [x]  Проанализированы распределения размеров гранул на обучающей и тестовой выборках\n",
    "    - [x]  Исследованы суммарные концентрации\n",
    "- [x]  Выполнен шаг 3: построена модель прогнозирования\n",
    "    - [x]  Написана функция для вычисления итогового *sMAPE*\n",
    "    - [x]  Обучено и проверено несколько моделей\n",
    "    - [x]  Выбрана лучшая модель, её качество проверено на тестовой выборке"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 43797,
    "start_time": "2022-04-18T13:20:07.015Z"
   },
   {
    "duration": 42,
    "start_time": "2022-04-18T13:25:07.709Z"
   },
   {
    "duration": 468,
    "start_time": "2022-04-18T13:25:10.051Z"
   },
   {
    "duration": 26,
    "start_time": "2022-04-18T13:25:12.083Z"
   },
   {
    "duration": 649,
    "start_time": "2022-04-18T13:25:29.522Z"
   },
   {
    "duration": 17,
    "start_time": "2022-04-18T13:25:44.074Z"
   },
   {
    "duration": 45,
    "start_time": "2022-04-18T13:26:53.830Z"
   },
   {
    "duration": 16,
    "start_time": "2022-04-18T13:27:20.765Z"
   },
   {
    "duration": 22,
    "start_time": "2022-04-18T13:27:55.541Z"
   },
   {
    "duration": 5,
    "start_time": "2022-04-18T13:41:56.218Z"
   },
   {
    "duration": 14,
    "start_time": "2022-04-18T13:43:55.934Z"
   },
   {
    "duration": 5,
    "start_time": "2022-04-18T13:54:02.447Z"
   },
   {
    "duration": 5,
    "start_time": "2022-04-18T13:54:55.803Z"
   },
   {
    "duration": 8,
    "start_time": "2022-04-18T13:55:05.470Z"
   },
   {
    "duration": 7,
    "start_time": "2022-04-18T13:55:15.638Z"
   },
   {
    "duration": 9,
    "start_time": "2022-04-18T13:57:11.561Z"
   },
   {
    "duration": 623,
    "start_time": "2022-04-18T13:58:40.684Z"
   },
   {
    "duration": 58,
    "start_time": "2022-04-18T14:00:10.265Z"
   },
   {
    "duration": 19,
    "start_time": "2022-04-18T14:01:20.300Z"
   },
   {
    "duration": 7,
    "start_time": "2022-04-18T14:01:23.567Z"
   },
   {
    "duration": 123,
    "start_time": "2022-04-18T14:02:32.298Z"
   },
   {
    "duration": 136,
    "start_time": "2022-04-18T14:03:13.711Z"
   },
   {
    "duration": 128,
    "start_time": "2022-04-18T14:03:17.463Z"
   },
   {
    "duration": 8,
    "start_time": "2022-04-18T14:03:53.623Z"
   },
   {
    "duration": 6,
    "start_time": "2022-04-18T14:04:07.172Z"
   },
   {
    "duration": 5,
    "start_time": "2022-04-18T14:09:18.518Z"
   },
   {
    "duration": 4,
    "start_time": "2022-04-18T14:11:20.098Z"
   },
   {
    "duration": 3,
    "start_time": "2022-04-18T14:11:35.519Z"
   },
   {
    "duration": 13,
    "start_time": "2022-04-18T14:11:53.182Z"
   },
   {
    "duration": 4,
    "start_time": "2022-04-18T14:12:53.288Z"
   },
   {
    "duration": 4,
    "start_time": "2022-04-18T14:13:20.483Z"
   },
   {
    "duration": 11,
    "start_time": "2022-04-18T14:13:34.474Z"
   },
   {
    "duration": 16,
    "start_time": "2022-04-18T14:14:10.098Z"
   },
   {
    "duration": 5,
    "start_time": "2022-04-18T14:14:24.401Z"
   },
   {
    "duration": 8,
    "start_time": "2022-04-18T14:14:35.999Z"
   },
   {
    "duration": 6,
    "start_time": "2022-04-18T14:17:34.671Z"
   },
   {
    "duration": 5,
    "start_time": "2022-04-18T14:18:40.084Z"
   },
   {
    "duration": 6,
    "start_time": "2022-04-18T14:19:07.970Z"
   },
   {
    "duration": 4,
    "start_time": "2022-04-18T14:21:10.277Z"
   },
   {
    "duration": 28,
    "start_time": "2022-04-18T14:24:29.612Z"
   },
   {
    "duration": 15,
    "start_time": "2022-04-18T14:24:38.170Z"
   },
   {
    "duration": 2588,
    "start_time": "2022-04-18T15:06:08.506Z"
   },
   {
    "duration": 10,
    "start_time": "2022-04-18T15:16:49.336Z"
   },
   {
    "duration": 8,
    "start_time": "2022-04-18T15:17:25.980Z"
   },
   {
    "duration": 4,
    "start_time": "2022-04-18T15:17:33.570Z"
   },
   {
    "duration": 10,
    "start_time": "2022-04-18T15:18:04.474Z"
   },
   {
    "duration": 4,
    "start_time": "2022-04-18T15:18:17.984Z"
   },
   {
    "duration": 11,
    "start_time": "2022-04-18T15:18:52.127Z"
   },
   {
    "duration": 4,
    "start_time": "2022-04-18T15:19:10.878Z"
   },
   {
    "duration": 3,
    "start_time": "2022-04-18T15:21:09.097Z"
   },
   {
    "duration": 4,
    "start_time": "2022-04-18T15:21:27.496Z"
   },
   {
    "duration": 119,
    "start_time": "2022-04-18T15:22:25.142Z"
   },
   {
    "duration": 120,
    "start_time": "2022-04-18T15:22:46.244Z"
   },
   {
    "duration": 114,
    "start_time": "2022-04-18T15:22:53.795Z"
   },
   {
    "duration": 115,
    "start_time": "2022-04-18T15:23:34.378Z"
   },
   {
    "duration": 7,
    "start_time": "2022-04-18T15:24:31.751Z"
   },
   {
    "duration": 4,
    "start_time": "2022-04-18T15:30:36.489Z"
   },
   {
    "duration": 4,
    "start_time": "2022-04-18T15:30:51.117Z"
   },
   {
    "duration": 4,
    "start_time": "2022-04-18T15:31:20.181Z"
   },
   {
    "duration": 8,
    "start_time": "2022-04-18T15:32:03.795Z"
   },
   {
    "duration": 12,
    "start_time": "2022-04-18T15:32:16.809Z"
   },
   {
    "duration": 7,
    "start_time": "2022-04-18T15:33:07.440Z"
   },
   {
    "duration": 8,
    "start_time": "2022-04-18T15:33:44.862Z"
   },
   {
    "duration": 7,
    "start_time": "2022-04-18T15:37:52.873Z"
   },
   {
    "duration": 7,
    "start_time": "2022-04-18T15:38:33.384Z"
   },
   {
    "duration": 11,
    "start_time": "2022-04-18T15:40:31.779Z"
   },
   {
    "duration": 8,
    "start_time": "2022-04-18T15:40:41.281Z"
   },
   {
    "duration": 9,
    "start_time": "2022-04-18T15:40:52.272Z"
   },
   {
    "duration": 8,
    "start_time": "2022-04-18T15:41:05.225Z"
   },
   {
    "duration": 14,
    "start_time": "2022-04-18T15:43:52.296Z"
   },
   {
    "duration": 16,
    "start_time": "2022-04-18T15:44:39.685Z"
   },
   {
    "duration": 8,
    "start_time": "2022-04-18T15:45:54.092Z"
   },
   {
    "duration": 12,
    "start_time": "2022-04-18T15:46:10.508Z"
   },
   {
    "duration": 10,
    "start_time": "2022-04-18T15:46:47.464Z"
   },
   {
    "duration": 13,
    "start_time": "2022-04-18T15:48:29.972Z"
   },
   {
    "duration": 17,
    "start_time": "2022-04-18T15:48:51.577Z"
   },
   {
    "duration": 17,
    "start_time": "2022-04-18T15:49:18.033Z"
   },
   {
    "duration": 12,
    "start_time": "2022-04-18T15:49:26.348Z"
   },
   {
    "duration": 502,
    "start_time": "2022-04-18T15:49:32.879Z"
   },
   {
    "duration": 114,
    "start_time": "2022-04-18T15:50:14.222Z"
   },
   {
    "duration": 101,
    "start_time": "2022-04-18T15:51:36.883Z"
   },
   {
    "duration": 134,
    "start_time": "2022-04-18T15:51:50.794Z"
   },
   {
    "duration": 190,
    "start_time": "2022-04-18T15:52:16.078Z"
   },
   {
    "duration": 101,
    "start_time": "2022-04-18T15:52:36.661Z"
   },
   {
    "duration": 3,
    "start_time": "2022-04-18T15:53:38.380Z"
   },
   {
    "duration": 260,
    "start_time": "2022-04-18T15:53:43.308Z"
   },
   {
    "duration": 267,
    "start_time": "2022-04-18T15:53:53.748Z"
   },
   {
    "duration": 1499,
    "start_time": "2022-04-19T13:34:25.484Z"
   },
   {
    "duration": 96,
    "start_time": "2022-04-19T13:36:15.971Z"
   },
   {
    "duration": 3,
    "start_time": "2022-04-19T13:36:20.304Z"
   },
   {
    "duration": 619,
    "start_time": "2022-04-19T13:36:20.309Z"
   },
   {
    "duration": 27,
    "start_time": "2022-04-19T13:36:20.930Z"
   },
   {
    "duration": 1404,
    "start_time": "2022-04-19T13:36:27.689Z"
   },
   {
    "duration": 22,
    "start_time": "2022-04-19T13:38:16.617Z"
   },
   {
    "duration": 25,
    "start_time": "2022-04-19T13:38:22.387Z"
   },
   {
    "duration": 23,
    "start_time": "2022-04-19T13:38:29.469Z"
   },
   {
    "duration": 1528,
    "start_time": "2022-04-19T13:39:35.164Z"
   },
   {
    "duration": 1605,
    "start_time": "2022-04-19T13:39:58.761Z"
   },
   {
    "duration": 6,
    "start_time": "2022-04-19T13:40:12.064Z"
   },
   {
    "duration": 27,
    "start_time": "2022-04-19T13:40:27.727Z"
   },
   {
    "duration": 23,
    "start_time": "2022-04-19T13:40:33.755Z"
   },
   {
    "duration": 19,
    "start_time": "2022-04-19T13:41:15.893Z"
   },
   {
    "duration": 28,
    "start_time": "2022-04-19T13:41:24.500Z"
   },
   {
    "duration": 30,
    "start_time": "2022-04-19T13:41:39.403Z"
   },
   {
    "duration": 30,
    "start_time": "2022-04-19T14:02:04.450Z"
   },
   {
    "duration": 83,
    "start_time": "2022-04-19T14:02:17.225Z"
   },
   {
    "duration": 23,
    "start_time": "2022-04-19T14:02:28.878Z"
   },
   {
    "duration": 19,
    "start_time": "2022-04-19T14:03:00.471Z"
   },
   {
    "duration": 20,
    "start_time": "2022-04-19T14:06:02.127Z"
   },
   {
    "duration": 13,
    "start_time": "2022-04-19T14:06:14.845Z"
   },
   {
    "duration": 13,
    "start_time": "2022-04-19T14:06:23.709Z"
   },
   {
    "duration": 267,
    "start_time": "2022-04-19T14:06:28.525Z"
   },
   {
    "duration": 284,
    "start_time": "2022-04-19T14:06:34.620Z"
   },
   {
    "duration": 13,
    "start_time": "2022-04-19T14:11:46.121Z"
   },
   {
    "duration": 295,
    "start_time": "2022-04-19T14:16:44.569Z"
   },
   {
    "duration": 306,
    "start_time": "2022-04-19T14:17:04.719Z"
   },
   {
    "duration": 385,
    "start_time": "2022-04-19T14:17:26.783Z"
   },
   {
    "duration": 506,
    "start_time": "2022-04-19T14:17:43.406Z"
   },
   {
    "duration": 614,
    "start_time": "2022-04-19T14:25:42.276Z"
   },
   {
    "duration": 507,
    "start_time": "2022-04-19T14:26:44.901Z"
   },
   {
    "duration": 320,
    "start_time": "2022-04-19T14:30:37.522Z"
   },
   {
    "duration": 3,
    "start_time": "2022-04-19T14:32:44.860Z"
   },
   {
    "duration": 5,
    "start_time": "2022-04-19T14:32:52.179Z"
   },
   {
    "duration": 6,
    "start_time": "2022-04-19T14:35:00.134Z"
   },
   {
    "duration": 6,
    "start_time": "2022-04-19T14:35:14.013Z"
   },
   {
    "duration": 7,
    "start_time": "2022-04-19T14:36:06.572Z"
   },
   {
    "duration": 10,
    "start_time": "2022-04-19T14:36:12.266Z"
   },
   {
    "duration": 6,
    "start_time": "2022-04-19T14:37:56.597Z"
   },
   {
    "duration": 7,
    "start_time": "2022-04-19T14:38:21.732Z"
   },
   {
    "duration": 2447,
    "start_time": "2022-04-19T14:39:13.610Z"
   },
   {
    "duration": 2398,
    "start_time": "2022-04-19T14:40:13.560Z"
   },
   {
    "duration": 2079,
    "start_time": "2022-04-19T14:41:08.077Z"
   },
   {
    "duration": 6,
    "start_time": "2022-04-19T14:41:28.772Z"
   },
   {
    "duration": 7,
    "start_time": "2022-04-19T14:54:55.075Z"
   },
   {
    "duration": 11,
    "start_time": "2022-04-19T14:55:04.213Z"
   },
   {
    "duration": 9,
    "start_time": "2022-04-19T14:56:09.826Z"
   },
   {
    "duration": 7,
    "start_time": "2022-04-19T14:59:17.487Z"
   },
   {
    "duration": 113,
    "start_time": "2022-04-19T14:59:30.025Z"
   },
   {
    "duration": 104,
    "start_time": "2022-04-19T15:00:47.806Z"
   },
   {
    "duration": 262,
    "start_time": "2022-04-19T15:02:06.995Z"
   },
   {
    "duration": 98,
    "start_time": "2022-04-19T15:04:23.100Z"
   },
   {
    "duration": 22,
    "start_time": "2022-04-19T16:13:12.098Z"
   },
   {
    "duration": 9,
    "start_time": "2022-04-19T16:13:50.870Z"
   },
   {
    "duration": 3,
    "start_time": "2022-04-19T16:13:56.366Z"
   },
   {
    "duration": 615,
    "start_time": "2022-04-19T16:13:58.396Z"
   },
   {
    "duration": 9,
    "start_time": "2022-04-19T16:14:03.045Z"
   },
   {
    "duration": 3,
    "start_time": "2022-04-19T16:14:12.653Z"
   },
   {
    "duration": 4,
    "start_time": "2022-04-19T16:14:29.218Z"
   },
   {
    "duration": 18,
    "start_time": "2022-04-19T16:14:39.890Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
